{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xy2119/Causal_Knowledge_GNN/blob/main/notebooks/3_causal_weighting_embedding(10d)_ate(13d).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook uses GCN and GAT to Estimate Average Treatment Effect based on BN Connectivity and Causally Weighted Feature Embeddings"
      ],
      "metadata": {
        "id": "gE7GN3zefsVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypgk9VLvSyWC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from gensim.models import word2vec\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import random\n",
        "random.seed(2022)\n",
        "np.random.seed(2022)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvg78-OAfWzb",
        "outputId": "59a37daa-e786-45c4-df9f-e9598f62d87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qlU_k_HFkNy"
      },
      "outputs": [],
      "source": [
        "df_train=pd.read_csv(\"criteo_sampled/criteo_train.csv\",index_col=0)\n",
        "df_test=pd.read_csv(\"criteo_sampled/criteo_test.csv\",index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI3sbjmrSyWF",
        "outputId": "317acbb6-c602-43be-fc2a-63bc904637f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-0.01388679662724193,\n",
              " 0.05071538371534517,\n",
              " 0.001328853128405821,\n",
              " -0.009331114629769072,\n",
              " 0.1933108932875426,\n",
              " 0.0008912893595676433,\n",
              " -0.0002958249515911775,\n",
              " -0.009201954095841196,\n",
              " 0.1034574925533489,\n",
              " 0.0741312293943781,\n",
              " -0.01664079180275199,\n",
              " 0.1331562899243786,\n",
              " 0.9310119697738223,\n",
              " 0.007447354046319239]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load causal weighting\n",
        "ate_list=[]\n",
        "ate=pd.read_excel(\"feats_ate_x13.xlsx\",index_col=0)\n",
        "for i in [c for c in df_train.columns[:-4]]+[\"visit\",\"treatment\"]:\n",
        "    ate_list.append(float(ate[ate['Feature']==i][\"ATE\"].values))\n",
        "ate_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXWeRuMvFkN1"
      },
      "outputs": [],
      "source": [
        "# load edge index\n",
        "edge_index=pd.read_csv('edge_index_criteo.csv')\n",
        "edge_index=torch.from_numpy(np.transpose(np.array(edge_index)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-pWo7KzFkN1",
        "outputId": "ae7ec32a-143f-4ec3-c0d3-52f70ee54b5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load node embedding\n",
        "model_dw=word2vec.Word2Vec.load(\"deepwalk_10d_x13.model\")\n",
        "model_n2v=word2vec.Word2Vec.load(\"Node2Vec_10d_x13.model\")\n",
        "\n",
        "lst_dw=[]\n",
        "lst_n2v=[]\n",
        "for i in range(14):\n",
        "    lst_dw.append(model_dw.wv[i])\n",
        "    lst_n2v.append(model_n2v.wv[i])\n",
        "len(lst_n2v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gyw5M55FkN2"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import entropy\n",
        "def R2(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    R2 = 1 - np.sum(np.square(y_actual-y_predicted)) / np.sum(np.square(y_actual-np.mean(y_actual)))\n",
        "    return R2\n",
        "\n",
        "def MAE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MAE = np.mean(abs(y_actual-y_predicted))\n",
        "    return MAE\n",
        "\n",
        "def RMSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    RMSE = np.sqrt(np.mean(np.square(y_actual-y_predicted)))\n",
        "    return RMSE\n",
        "\n",
        "def CVRMSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    RMSE = np.sqrt(np.mean(np.square(y_actual-y_predicted)))\n",
        "    CVRMSE = RMSE/np.mean(y_actual)\n",
        "    return CVRMSE\n",
        "\n",
        "def MAPE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MAPE=np.mean(abs((y_actual-y_predicted)/y_actual))\n",
        "    return MAPE\n",
        "\n",
        "def MSE(y_predicted, y_actual):\n",
        "    y_predicted = np.asarray(y_predicted, dtype=float)\n",
        "    y_actual = np.asarray(y_actual, dtype=float)\n",
        "\n",
        "    MSE = np.mean(abs(y_actual-y_predicted)**2)\n",
        "    return MSE\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "\n",
        "    p = np.asarray(p, dtype=float)\n",
        "    q = np.asarray(q, dtype=float)\n",
        "    return np.sum(np.where(p != 0, p * np.log(p / q), 0))\n",
        "\n",
        "def kl_divergence(y_predicted, y_actual):\n",
        "\n",
        "    stacked_values = np.hstack((y_predicted, y_actual))\n",
        "    stacked_low = np.percentile(stacked_values, 0.1)\n",
        "    stacked_high = np.percentile(stacked_values, 99.9)\n",
        "    bins = np.linspace(stacked_low, stacked_high, 100)\n",
        "\n",
        "    distr = np.histogram(y_predicted, bins=bins)[0]\n",
        "    distr = np.clip(distr / distr.sum(), 0.001, 0.999)\n",
        "    true_distr = np.histogram( y_actual, bins=bins)[0]\n",
        "    true_distr = np.clip(true_distr / true_distr.sum(), 0.001, 0.999)\n",
        "\n",
        "    kl = entropy(distr, true_distr)\n",
        "    return kl "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "H3emQp0YFkN3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.data import InMemoryDataset, download_url\n",
        "\n",
        "\n",
        "class Mytrain(InMemoryDataset):\n",
        "    def __init__(self, root,transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "       # ,self.edge_index,self.y=x,edge_index,y\n",
        "        \n",
        "    \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['some_file_1', 'some_file_2', ...]\n",
        "    \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "        data_list=[]\n",
        "        weighted_feats=[]\n",
        "        for i in range(x_train.shape[0]):\n",
        "            Edge_index = edge_index.type(torch.long)\n",
        "            X =x_train[i]\n",
        "            if feats_mode =='causal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                causal_weighted= np.multiply(np.array(X),np.array(ate_list))\n",
        "                weighted_feats.append(causal_weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(causal_weighted) # feature (10d), causally weighted features        \n",
        "                                                        ))) \n",
        "            elif feats_mode =='equal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                weighted= np.array(X)\n",
        "                weighted_feats.append(weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(weighted) # feature (10d), equally weighted features        \n",
        "                                                        ))) \n",
        "            else:\n",
        "                t=torch.zeros(14,11) # 11 dimensions = 1+10 = node number, node embedding\n",
        "                                    \n",
        "                \n",
        "                for j in range(14):\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        ))) \n",
        "                  \n",
        "\n",
        "            Y = y_train[i].reshape(-1,1).to(torch.float32)\n",
        "            data = Data(x=t, edge_index=Edge_index, y=Y)\n",
        "      \n",
        "            data_list.append(data)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])\n",
        "\n",
        "\n",
        "class Mytest(InMemoryDataset):\n",
        "    def __init__(self, root,transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "       # ,self.edge_index,self.y=x,edge_index,y\n",
        "        \n",
        "    \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return ['some_file_1', 'some_file_2', ...]\n",
        "    \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data.pt']\n",
        "\n",
        "    def process(self):\n",
        "        data_list=[]\n",
        "        weighted_feats=[]\n",
        "        for i in range(x_test.shape[0]):\n",
        "            Edge_index = edge_index.type(torch.long)\n",
        "            X =x_test[i]\n",
        "\n",
        "            if feats_mode =='causal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                causal_weighted= np.multiply(np.array(X),np.array(ate_list))\n",
        "                weighted_feats.append(causal_weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(causal_weighted) # feature (10d), causally weighted features        \n",
        "                                                        ))) \n",
        "            elif feats_mode =='equal':\n",
        "                t=torch.zeros(14,25) # 25 dimensions = 1+10+14 = node number, node embedding, features\n",
        "              \n",
        "                weighted= np.array(X)\n",
        "                weighted_feats.append(weighted)\n",
        "                \n",
        "                for j in range(14): # for j-th feature\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        list(weighted) # feature (10d), equally weighted features        \n",
        "                                                        ))) \n",
        "            else:\n",
        "                t=torch.zeros(14,11) # 11 dimensions = 1+10 = node number, node embedding\n",
        "                                    \n",
        "                \n",
        "                for j in range(14):\n",
        "                  t[j]=torch.from_numpy(np.concatenate(( [float(X[int(j)])], # node (1d)\n",
        "                                                        list(lst_dw[int(j)]), # node embeddings (10d), prior embeddings from DeepWalk/Node2Vec\n",
        "                                                        ))) \n",
        "            \n",
        "            Y = y_test[i].reshape(-1,1).to(torch.float32)\n",
        "            data = Data(x=t, edge_index=Edge_index, y=Y)\n",
        "        \n",
        "            data_list.append(data)\n",
        "\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feats_mode='causal'\n",
        "#feats_mode ='equal' \n",
        "#feats_mode ='noweighting' \n",
        "\n",
        "col=df_train.columns\n",
        "y_train=torch.from_numpy(np.array(df_train['y'])).reshape(df_train.shape[0],1).to(torch.float32)\n",
        "x_train=torch.from_numpy(np.array(df_train[[i for i in col[:-4]]+[\"visit\",\"T\"]])).to(torch.float32)\n",
        "Mydata_train=Mytrain(\".\\mydata_sampled_ate\\MYdata_train\")\n",
        "\n",
        "y_test=torch.from_numpy(np.array(df_test['y'])).reshape(df_test.shape[0],1).to(torch.float32)\n",
        "x_test=torch.from_numpy(np.array(df_test[[i for i in col[:-4]]+[\"visit\",\"T\"]])).to(torch.float32)\n",
        "Mydata_test=Mytest(\".\\mydata_sampled_ate\\MYdata_test\")"
      ],
      "metadata": {
        "id": "vpIiW0Npfq5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dDvnHBgFkN6"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKmQvnebFkN7"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in train_loader:\n",
        "        loss = 0\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        label = data.y.to(device)\n",
        "        loss = crit(output, label)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "        y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "        y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "    \n",
        "    \n",
        "    loss=loss_all/len(Mydata_train)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2,mse,kl\n",
        "\n",
        "\n",
        "def val():\n",
        "    model.eval()\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in test_loader:\n",
        "      loss = 0\n",
        "      data = data.to(device)\n",
        "      output = model(data)\n",
        "      label = data.y.to(device)\n",
        "      y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "      y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "      loss = crit(output, label)\n",
        "      loss_all += loss.item()\n",
        "\n",
        "    \n",
        "    loss = loss_all / len(Mydata_test)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2, mse,kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8VbbpTvFkN8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_scatter import scatter_add\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GCNConv(25, 64)\n",
        "        self.conv2 = GCNConv(64, 10)\n",
        "        # self-attention layer\n",
        "        \n",
        "        self.f1 = torch.nn.Linear(140,32)\n",
        "        self.f2 = torch.nn.Linear(32,1)\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.attention\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = x.reshape(-1,140)\n",
        "        x = self.f1(x)\n",
        "        x = self.f2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvzm6UHUFkN8"
      },
      "outputs": [],
      "source": [
        "num_epochs = 2560\n",
        "batch_size = 512\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
        "crit = F.mse_loss\n",
        "train_loader = DataLoader(Mydata_train, batch_size=batch_size)\n",
        "test_loader = DataLoader(Mydata_test, batch_size=512)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss,r2,mse,kl=train()\n",
        "    if epoch %5==0:      \n",
        "        print('train_loss:')\n",
        "        print(loss)\n",
        "\n",
        "        loss,r2,mse,kl=val()\n",
        "        print('test_loss:')\n",
        "        print(loss)\n",
        "\n",
        "y_predicted = []\n",
        "for data in test_loader:\n",
        "    loss = 0\n",
        "    data = data.to(device)\n",
        "    output = model(data)\n",
        "    label = data.y.to(device)\n",
        "    y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "df_test[\"y_hat\"]=y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhsJfdeIFkN9"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(Mydata_train, batch_size=batch_size)\n",
        "for epoch in range(num_epochs):\n",
        "    loss,r2,mse,kl=train()\n",
        "    if epoch %5==0:      \n",
        "        print('train_loss:')\n",
        "        print(loss)\n",
        "\n",
        "        loss,r2,mse,kl=val()\n",
        "        print('test_loss:')\n",
        "        print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezCCwtkgFkN_",
        "outputId": "3d36ffca-1759-428f-e452-5c169b248ce9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auuc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_hat</th>\n",
              "      <td>0.732616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.499185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            auuc\n",
              "y_hat   0.732616\n",
              "Random  0.499185"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# estimate area under the uplift curve (AUUC)\n",
        "from causalml.metrics import *\n",
        "uplift=df_test.copy()\n",
        "tau_hat=pd.concat([t0,t1], axis=0, join=\"inner\")\n",
        "uplift=pd.concat([uplift,tau_hat], axis=1, join=\"inner\")\n",
        "uplift = uplift.loc[:,~uplift.columns.duplicated()]\n",
        "\n",
        "auuc=auuc_score(uplift, outcome_col='y', treatment_col='T')\n",
        "gcn_auuc=pd.DataFrame(auuc[[\"y_hat\",\"Random\"]],columns=['auuc'])\n",
        "gcn_auuc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGF8AUtZFkN_"
      },
      "source": [
        "# GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1jwzm-7FkN_",
        "outputId": "cbcfc005-886e-4d9c-c50c-2dd5ccb5394c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:-0.813384  MSE:0.005295  KL:2.094833  MAE:0.017525  RMSE:0.072765  CVRMSE:24.848187  train_loss:\n",
            "0.005294719280111404\n",
            "R2:0.072054  MSE:0.002798  KL:0.126590  MAE:0.005984  RMSE:0.052893  CVRMSE:17.490782  test_loss:\n",
            "5.497692996261892e-06\n",
            "R2:0.060488  MSE:0.002743  KL:2.104842  MAE:0.009919  RMSE:0.052375  CVRMSE:17.885496  R2:0.057913  MSE:0.002751  KL:1.238602  MAE:0.008663  RMSE:0.052447  CVRMSE:17.909985  R2:0.060253  MSE:0.002744  KL:1.248182  MAE:0.008433  RMSE:0.052382  CVRMSE:17.887734  R2:0.064215  MSE:0.002732  KL:1.533469  MAE:0.008456  RMSE:0.052271  CVRMSE:17.849982  R2:0.064579  MSE:0.002731  KL:1.586445  MAE:0.008461  RMSE:0.052261  CVRMSE:17.846508  train_loss:\n",
            "0.0027312411740449236\n",
            "R2:0.056647  MSE:0.002844  KL:0.100752  MAE:0.005517  RMSE:0.053330  CVRMSE:17.635387  test_loss:\n",
            "5.589489941400884e-06\n",
            "R2:0.064812  MSE:0.002731  KL:1.744580  MAE:0.008459  RMSE:0.052255  CVRMSE:17.844289  R2:0.065108  MSE:0.002730  KL:1.738378  MAE:0.008456  RMSE:0.052247  CVRMSE:17.841467  R2:0.064888  MSE:0.002730  KL:1.717766  MAE:0.008453  RMSE:0.052253  CVRMSE:17.843561  R2:0.064689  MSE:0.002731  KL:1.686471  MAE:0.008447  RMSE:0.052258  CVRMSE:17.845459  R2:0.064419  MSE:0.002732  KL:1.771654  MAE:0.008445  RMSE:0.052266  CVRMSE:17.848038  train_loss:\n",
            "0.0027317093230096023\n",
            "R2:0.057155  MSE:0.002843  KL:0.105881  MAE:0.005612  RMSE:0.053316  CVRMSE:17.630644  test_loss:\n",
            "5.586452004709481e-06\n",
            "R2:0.064848  MSE:0.002730  KL:1.700174  MAE:0.008435  RMSE:0.052254  CVRMSE:17.843949  R2:0.064525  MSE:0.002731  KL:1.587066  MAE:0.008468  RMSE:0.052263  CVRMSE:17.847029  R2:0.064762  MSE:0.002731  KL:1.600062  MAE:0.008477  RMSE:0.052256  CVRMSE:17.844764  R2:0.064245  MSE:0.002732  KL:1.584316  MAE:0.008466  RMSE:0.052271  CVRMSE:17.849697  R2:0.064774  MSE:0.002731  KL:1.571855  MAE:0.008457  RMSE:0.052256  CVRMSE:17.844656  train_loss:\n",
            "0.0027306743580779755\n",
            "R2:0.057591  MSE:0.002841  KL:0.101139  MAE:0.005591  RMSE:0.053304  CVRMSE:17.626561  test_loss:\n",
            "5.583864519005802e-06\n",
            "R2:0.064878  MSE:0.002730  KL:1.737214  MAE:0.008471  RMSE:0.052253  CVRMSE:17.843661  R2:0.064846  MSE:0.002730  KL:1.639765  MAE:0.008485  RMSE:0.052254  CVRMSE:17.843967  R2:0.064428  MSE:0.002732  KL:1.637591  MAE:0.008440  RMSE:0.052266  CVRMSE:17.847957  R2:0.065146  MSE:0.002730  KL:1.762952  MAE:0.008471  RMSE:0.052245  CVRMSE:17.841106  R2:0.064568  MSE:0.002731  KL:1.704276  MAE:0.008404  RMSE:0.052262  CVRMSE:17.846619  train_loss:\n",
            "0.002731274975775923\n",
            "R2:0.057446  MSE:0.002842  KL:0.101947  MAE:0.005929  RMSE:0.053308  CVRMSE:17.627918  test_loss:\n",
            "5.584688566759714e-06\n",
            "R2:0.064857  MSE:0.002730  KL:1.758027  MAE:0.008460  RMSE:0.052254  CVRMSE:17.843857  R2:0.064371  MSE:0.002732  KL:1.720052  MAE:0.008430  RMSE:0.052267  CVRMSE:17.848500  R2:0.065613  MSE:0.002728  KL:1.600872  MAE:0.008512  RMSE:0.052232  CVRMSE:17.836646  R2:0.064834  MSE:0.002730  KL:1.677273  MAE:0.008472  RMSE:0.052254  CVRMSE:17.844078  R2:0.064796  MSE:0.002731  KL:1.681250  MAE:0.008489  RMSE:0.052255  CVRMSE:17.844446  train_loss:\n",
            "0.002730610110640133\n",
            "R2:0.056179  MSE:0.002846  KL:0.102293  MAE:0.005667  RMSE:0.053344  CVRMSE:17.639761  test_loss:\n",
            "5.592255682604433e-06\n",
            "R2:0.065086  MSE:0.002730  KL:1.751158  MAE:0.008489  RMSE:0.052247  CVRMSE:17.841678  R2:0.065139  MSE:0.002730  KL:1.681575  MAE:0.008498  RMSE:0.052246  CVRMSE:17.841166  R2:0.064590  MSE:0.002731  KL:1.666795  MAE:0.008445  RMSE:0.052261  CVRMSE:17.846411  R2:0.064134  MSE:0.002733  KL:1.727385  MAE:0.008471  RMSE:0.052274  CVRMSE:17.850753  R2:0.064288  MSE:0.002732  KL:1.771836  MAE:0.008466  RMSE:0.052269  CVRMSE:17.849289  train_loss:\n",
            "0.002732092466848261\n",
            "R2:0.056223  MSE:0.002845  KL:0.101320  MAE:0.005534  RMSE:0.053342  CVRMSE:17.639352  test_loss:\n",
            "5.592012955994695e-06\n",
            "R2:0.064721  MSE:0.002731  KL:1.690621  MAE:0.008480  RMSE:0.052257  CVRMSE:17.845161  R2:0.064351  MSE:0.002732  KL:1.781035  MAE:0.008435  RMSE:0.052268  CVRMSE:17.848689  R2:0.065132  MSE:0.002730  KL:1.767785  MAE:0.008495  RMSE:0.052246  CVRMSE:17.841232  R2:0.064726  MSE:0.002731  KL:1.778767  MAE:0.008454  RMSE:0.052257  CVRMSE:17.845108  R2:0.064703  MSE:0.002731  KL:1.771874  MAE:0.008453  RMSE:0.052258  CVRMSE:17.845326  train_loss:\n",
            "0.0027308791713068718\n",
            "R2:0.057436  MSE:0.002842  KL:0.101456  MAE:0.005700  RMSE:0.053308  CVRMSE:17.628015  test_loss:\n",
            "5.58477663659164e-06\n",
            "R2:0.064470  MSE:0.002732  KL:1.705905  MAE:0.008504  RMSE:0.052264  CVRMSE:17.847553  R2:0.064227  MSE:0.002732  KL:1.700385  MAE:0.008481  RMSE:0.052271  CVRMSE:17.849874  R2:0.064825  MSE:0.002731  KL:1.662722  MAE:0.008450  RMSE:0.052254  CVRMSE:17.844161  R2:0.064688  MSE:0.002731  KL:1.691676  MAE:0.008421  RMSE:0.052258  CVRMSE:17.845468  R2:0.064823  MSE:0.002731  KL:1.694118  MAE:0.008454  RMSE:0.052254  CVRMSE:17.844189  train_loss:\n",
            "0.0027305313805355584\n",
            "R2:0.055858  MSE:0.002846  KL:0.101368  MAE:0.005397  RMSE:0.053353  CVRMSE:17.642758  test_loss:\n",
            "5.5941998764456e-06\n",
            "R2:0.064248  MSE:0.002732  KL:1.797117  MAE:0.008437  RMSE:0.052271  CVRMSE:17.849672  R2:0.064843  MSE:0.002730  KL:1.655213  MAE:0.008473  RMSE:0.052254  CVRMSE:17.843990  R2:0.064444  MSE:0.002732  KL:1.651202  MAE:0.008439  RMSE:0.052265  CVRMSE:17.847803  R2:0.064508  MSE:0.002731  KL:1.614444  MAE:0.008476  RMSE:0.052263  CVRMSE:17.847187  R2:0.064360  MSE:0.002732  KL:1.642559  MAE:0.008464  RMSE:0.052267  CVRMSE:17.848602  train_loss:\n",
            "0.0027318819484405702\n",
            "R2:0.055815  MSE:0.002847  KL:0.099192  MAE:0.005478  RMSE:0.053354  CVRMSE:17.643168  test_loss:\n",
            "5.594451637670164e-06\n",
            "R2:0.064737  MSE:0.002731  KL:1.703784  MAE:0.008439  RMSE:0.052257  CVRMSE:17.845003  R2:0.064424  MSE:0.002732  KL:1.719806  MAE:0.008453  RMSE:0.052266  CVRMSE:17.847992  R2:0.064858  MSE:0.002730  KL:1.760289  MAE:0.008442  RMSE:0.052254  CVRMSE:17.843853  R2:0.064447  MSE:0.002732  KL:1.555441  MAE:0.008456  RMSE:0.052265  CVRMSE:17.847773  R2:0.065163  MSE:0.002730  KL:1.668799  MAE:0.008479  RMSE:0.052245  CVRMSE:17.840941  train_loss:\n",
            "0.002729537395457832\n",
            "R2:0.057650  MSE:0.002841  KL:0.102368  MAE:0.005638  RMSE:0.053302  CVRMSE:17.626010  test_loss:\n",
            "5.583506820034072e-06\n",
            "R2:0.064775  MSE:0.002731  KL:1.590389  MAE:0.008488  RMSE:0.052256  CVRMSE:17.844646  R2:0.064599  MSE:0.002731  KL:1.794874  MAE:0.008455  RMSE:0.052261  CVRMSE:17.846317  R2:0.064761  MSE:0.002731  KL:1.739834  MAE:0.008467  RMSE:0.052256  CVRMSE:17.844779  R2:0.064410  MSE:0.002732  KL:1.575354  MAE:0.008464  RMSE:0.052266  CVRMSE:17.848126  R2:0.064437  MSE:0.002732  KL:1.689963  MAE:0.008430  RMSE:0.052265  CVRMSE:17.847866  train_loss:\n",
            "0.002731656665804788\n",
            "R2:0.055927  MSE:0.002846  KL:0.099755  MAE:0.005441  RMSE:0.053351  CVRMSE:17.642119  test_loss:\n",
            "5.593786711159169e-06\n",
            "R2:0.064171  MSE:0.002732  KL:1.776943  MAE:0.008430  RMSE:0.052273  CVRMSE:17.850408  R2:0.064861  MSE:0.002730  KL:1.656352  MAE:0.008475  RMSE:0.052253  CVRMSE:17.843823  R2:0.064593  MSE:0.002731  KL:1.806040  MAE:0.008420  RMSE:0.052261  CVRMSE:17.846378  R2:0.064720  MSE:0.002731  KL:1.621032  MAE:0.008485  RMSE:0.052257  CVRMSE:17.845166  R2:0.063987  MSE:0.002733  KL:1.650493  MAE:0.008423  RMSE:0.052278  CVRMSE:17.852159  train_loss:\n",
            "0.002732970946834444\n",
            "R2:0.056401  MSE:0.002845  KL:0.101754  MAE:0.005423  RMSE:0.053337  CVRMSE:17.637684  test_loss:\n",
            "5.590964178772395e-06\n",
            "R2:0.064141  MSE:0.002733  KL:1.713022  MAE:0.008435  RMSE:0.052274  CVRMSE:17.850694  R2:0.064961  MSE:0.002730  KL:1.662093  MAE:0.008489  RMSE:0.052251  CVRMSE:17.842868  R2:0.064520  MSE:0.002731  KL:1.641923  MAE:0.008471  RMSE:0.052263  CVRMSE:17.847071  R2:0.064870  MSE:0.002730  KL:1.722191  MAE:0.008447  RMSE:0.052253  CVRMSE:17.843740  R2:0.065105  MSE:0.002730  KL:1.701498  MAE:0.008500  RMSE:0.052247  CVRMSE:17.841497  train_loss:\n",
            "0.0027297074964130513\n",
            "R2:0.056810  MSE:0.002844  KL:0.104619  MAE:0.005862  RMSE:0.053326  CVRMSE:17.633864  test_loss:\n",
            "5.588473594955181e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064456  MSE:0.002732  KL:1.592869  MAE:0.008475  RMSE:0.052265  CVRMSE:17.847686  R2:0.064599  MSE:0.002731  KL:1.694082  MAE:0.008464  RMSE:0.052261  CVRMSE:17.846322  R2:0.064635  MSE:0.002731  KL:1.742830  MAE:0.008471  RMSE:0.052260  CVRMSE:17.845979  R2:0.064600  MSE:0.002731  KL:1.617236  MAE:0.008486  RMSE:0.052261  CVRMSE:17.846311  R2:0.064951  MSE:0.002730  KL:1.751280  MAE:0.008426  RMSE:0.052251  CVRMSE:17.842962  train_loss:\n",
            "0.002730155742312368\n",
            "R2:0.056884  MSE:0.002843  KL:0.100740  MAE:0.005550  RMSE:0.053324  CVRMSE:17.633171  test_loss:\n",
            "5.588077468592933e-06\n",
            "R2:0.064745  MSE:0.002731  KL:1.648253  MAE:0.008481  RMSE:0.052257  CVRMSE:17.844927  R2:0.064765  MSE:0.002731  KL:1.741393  MAE:0.008428  RMSE:0.052256  CVRMSE:17.844735  R2:0.065008  MSE:0.002730  KL:1.755607  MAE:0.008450  RMSE:0.052249  CVRMSE:17.842421  R2:0.064726  MSE:0.002731  KL:1.748324  MAE:0.008494  RMSE:0.052257  CVRMSE:17.845111  R2:0.064690  MSE:0.002731  KL:1.677063  MAE:0.008428  RMSE:0.052258  CVRMSE:17.845457  train_loss:\n",
            "0.002730919377886199\n",
            "R2:0.057752  MSE:0.002841  KL:0.101738  MAE:0.005950  RMSE:0.053299  CVRMSE:17.625056  test_loss:\n",
            "5.582871943918687e-06\n",
            "R2:0.064818  MSE:0.002731  KL:1.682847  MAE:0.008463  RMSE:0.052255  CVRMSE:17.844230  R2:0.064639  MSE:0.002731  KL:1.611868  MAE:0.008476  RMSE:0.052260  CVRMSE:17.845942  R2:0.064877  MSE:0.002730  KL:1.727803  MAE:0.008470  RMSE:0.052253  CVRMSE:17.843669  R2:0.064792  MSE:0.002731  KL:1.647050  MAE:0.008507  RMSE:0.052255  CVRMSE:17.844479  R2:0.064296  MSE:0.002732  KL:1.670514  MAE:0.008417  RMSE:0.052269  CVRMSE:17.849212  train_loss:\n",
            "0.0027320687873782336\n",
            "R2:0.056703  MSE:0.002844  KL:0.099643  MAE:0.005468  RMSE:0.053329  CVRMSE:17.634862  test_loss:\n",
            "5.5891627580563256e-06\n",
            "R2:0.064435  MSE:0.002732  KL:1.785619  MAE:0.008453  RMSE:0.052265  CVRMSE:17.847890  R2:0.064455  MSE:0.002732  KL:1.638296  MAE:0.008452  RMSE:0.052265  CVRMSE:17.847695  R2:0.064677  MSE:0.002731  KL:1.647132  MAE:0.008501  RMSE:0.052259  CVRMSE:17.845576  R2:0.064177  MSE:0.002732  KL:1.655989  MAE:0.008419  RMSE:0.052273  CVRMSE:17.850350  R2:0.064499  MSE:0.002731  KL:1.699778  MAE:0.008460  RMSE:0.052264  CVRMSE:17.847277  train_loss:\n",
            "0.0027314764952395573\n",
            "R2:0.056021  MSE:0.002846  KL:0.099867  MAE:0.005634  RMSE:0.053348  CVRMSE:17.641239  test_loss:\n",
            "5.593206878486648e-06\n",
            "R2:0.064360  MSE:0.002732  KL:1.766848  MAE:0.008418  RMSE:0.052267  CVRMSE:17.848604  R2:0.065001  MSE:0.002730  KL:1.761646  MAE:0.008470  RMSE:0.052249  CVRMSE:17.842486  R2:0.064799  MSE:0.002731  KL:1.598646  MAE:0.008444  RMSE:0.052255  CVRMSE:17.844410  R2:0.064598  MSE:0.002731  KL:1.779596  MAE:0.008444  RMSE:0.052261  CVRMSE:17.846327  R2:0.065238  MSE:0.002729  KL:1.784559  MAE:0.008451  RMSE:0.052243  CVRMSE:17.840222  train_loss:\n",
            "0.0027293174442505237\n",
            "R2:0.057232  MSE:0.002842  KL:0.099907  MAE:0.005697  RMSE:0.053314  CVRMSE:17.629924  test_loss:\n",
            "5.585997541951403e-06\n",
            "R2:0.065105  MSE:0.002730  KL:1.657976  MAE:0.008486  RMSE:0.052247  CVRMSE:17.841493  R2:0.064110  MSE:0.002733  KL:1.639189  MAE:0.008443  RMSE:0.052274  CVRMSE:17.850989  R2:0.064214  MSE:0.002732  KL:1.686351  MAE:0.008430  RMSE:0.052271  CVRMSE:17.849989  R2:0.064490  MSE:0.002732  KL:1.645759  MAE:0.008516  RMSE:0.052264  CVRMSE:17.847363  R2:0.064638  MSE:0.002731  KL:1.626688  MAE:0.008490  RMSE:0.052260  CVRMSE:17.845946  train_loss:\n",
            "0.0027310689594696774\n",
            "R2:0.055709  MSE:0.002847  KL:0.098952  MAE:0.005477  RMSE:0.053357  CVRMSE:17.644153  test_loss:\n",
            "5.595081306262181e-06\n",
            "R2:0.064428  MSE:0.002732  KL:1.654667  MAE:0.008476  RMSE:0.052266  CVRMSE:17.847951  R2:0.065021  MSE:0.002730  KL:1.610360  MAE:0.008462  RMSE:0.052249  CVRMSE:17.842298  R2:0.064430  MSE:0.002732  KL:1.718909  MAE:0.008440  RMSE:0.052265  CVRMSE:17.847933  R2:0.065105  MSE:0.002730  KL:1.674069  MAE:0.008469  RMSE:0.052247  CVRMSE:17.841495  R2:0.064707  MSE:0.002731  KL:1.719127  MAE:0.008444  RMSE:0.052258  CVRMSE:17.845289  train_loss:\n",
            "0.002730867949597845\n",
            "R2:0.058644  MSE:0.002838  KL:0.107497  MAE:0.005566  RMSE:0.053274  CVRMSE:17.616709  test_loss:\n",
            "5.5775863336024675e-06\n",
            "R2:0.064353  MSE:0.002732  KL:1.727802  MAE:0.008409  RMSE:0.052268  CVRMSE:17.848665  R2:0.064387  MSE:0.002732  KL:1.726828  MAE:0.008449  RMSE:0.052267  CVRMSE:17.848347  R2:0.064499  MSE:0.002731  KL:1.736829  MAE:0.008437  RMSE:0.052264  CVRMSE:17.847275  R2:0.065037  MSE:0.002730  KL:1.669933  MAE:0.008503  RMSE:0.052248  CVRMSE:17.842142  R2:0.064688  MSE:0.002731  KL:1.624539  MAE:0.008475  RMSE:0.052258  CVRMSE:17.845472  train_loss:\n",
            "0.0027309240503649882\n",
            "R2:0.057302  MSE:0.002842  KL:0.103007  MAE:0.005942  RMSE:0.053312  CVRMSE:17.629261  test_loss:\n",
            "5.585540136620533e-06\n",
            "R2:0.064534  MSE:0.002731  KL:1.610256  MAE:0.008472  RMSE:0.052263  CVRMSE:17.846942  R2:0.064132  MSE:0.002733  KL:1.626739  MAE:0.008468  RMSE:0.052274  CVRMSE:17.850777  R2:0.064346  MSE:0.002732  KL:1.635022  MAE:0.008432  RMSE:0.052268  CVRMSE:17.848739  R2:0.065261  MSE:0.002729  KL:1.727270  MAE:0.008456  RMSE:0.052242  CVRMSE:17.840005  R2:0.064944  MSE:0.002730  KL:1.641109  MAE:0.008486  RMSE:0.052251  CVRMSE:17.843026  train_loss:\n",
            "0.0027301755236011\n",
            "R2:0.058072  MSE:0.002840  KL:0.102273  MAE:0.005725  RMSE:0.053290  CVRMSE:17.622061  test_loss:\n",
            "5.580986415471979e-06\n",
            "R2:0.064712  MSE:0.002731  KL:1.655417  MAE:0.008482  RMSE:0.052258  CVRMSE:17.845246  R2:0.064145  MSE:0.002733  KL:1.641376  MAE:0.008444  RMSE:0.052273  CVRMSE:17.850651  R2:0.064958  MSE:0.002730  KL:1.688360  MAE:0.008464  RMSE:0.052251  CVRMSE:17.842900  R2:0.064856  MSE:0.002730  KL:1.657414  MAE:0.008444  RMSE:0.052254  CVRMSE:17.843870  R2:0.064869  MSE:0.002730  KL:1.729027  MAE:0.008463  RMSE:0.052253  CVRMSE:17.843741  train_loss:\n",
            "0.0027303943356395276\n",
            "R2:0.058536  MSE:0.002838  KL:0.105653  MAE:0.005822  RMSE:0.053277  CVRMSE:17.617721  test_loss:\n",
            "5.578207557727191e-06\n",
            "R2:0.065107  MSE:0.002730  KL:1.720739  MAE:0.008453  RMSE:0.052247  CVRMSE:17.841474  R2:0.064952  MSE:0.002730  KL:1.763707  MAE:0.008457  RMSE:0.052251  CVRMSE:17.842950  R2:0.065216  MSE:0.002729  KL:1.725312  MAE:0.008449  RMSE:0.052244  CVRMSE:17.840438  R2:0.064796  MSE:0.002731  KL:1.714522  MAE:0.008497  RMSE:0.052255  CVRMSE:17.844441  R2:0.064561  MSE:0.002731  KL:1.804187  MAE:0.008441  RMSE:0.052262  CVRMSE:17.846684  train_loss:\n",
            "0.002731294803988363\n",
            "R2:0.058216  MSE:0.002839  KL:0.105068  MAE:0.005601  RMSE:0.053286  CVRMSE:17.620717  test_loss:\n",
            "5.58013684098773e-06\n",
            "R2:0.064939  MSE:0.002730  KL:1.663067  MAE:0.008448  RMSE:0.052251  CVRMSE:17.843077  R2:0.064288  MSE:0.002732  KL:1.736287  MAE:0.008429  RMSE:0.052269  CVRMSE:17.849288  R2:0.064385  MSE:0.002732  KL:1.806806  MAE:0.008428  RMSE:0.052267  CVRMSE:17.848359  R2:0.064311  MSE:0.002732  KL:1.710324  MAE:0.008449  RMSE:0.052269  CVRMSE:17.849068  R2:0.065093  MSE:0.002730  KL:1.741876  MAE:0.008489  RMSE:0.052247  CVRMSE:17.841604  train_loss:\n",
            "0.0027297403431206925\n",
            "R2:0.057998  MSE:0.002840  KL:0.105469  MAE:0.005789  RMSE:0.053292  CVRMSE:17.622758  test_loss:\n",
            "5.581412084719874e-06\n",
            "R2:0.064229  MSE:0.002732  KL:1.734878  MAE:0.008424  RMSE:0.052271  CVRMSE:17.849851  R2:0.064454  MSE:0.002732  KL:1.699000  MAE:0.008432  RMSE:0.052265  CVRMSE:17.847706  R2:0.064650  MSE:0.002731  KL:1.603711  MAE:0.008456  RMSE:0.052259  CVRMSE:17.845839  R2:0.064931  MSE:0.002730  KL:1.741527  MAE:0.008466  RMSE:0.052251  CVRMSE:17.843153  R2:0.063759  MSE:0.002734  KL:1.648766  MAE:0.008450  RMSE:0.052284  CVRMSE:17.854333  train_loss:\n",
            "0.0027336366139315785\n",
            "R2:0.056284  MSE:0.002845  KL:0.101609  MAE:0.005656  RMSE:0.053341  CVRMSE:17.638776  test_loss:\n",
            "5.5916298944943015e-06\n",
            "R2:0.064626  MSE:0.002731  KL:1.791534  MAE:0.008473  RMSE:0.052260  CVRMSE:17.846065  R2:0.064404  MSE:0.002732  KL:1.754908  MAE:0.008428  RMSE:0.052266  CVRMSE:17.848186  R2:0.064342  MSE:0.002732  KL:1.615340  MAE:0.008433  RMSE:0.052268  CVRMSE:17.848769  R2:0.064916  MSE:0.002730  KL:1.763347  MAE:0.008433  RMSE:0.052252  CVRMSE:17.843301  R2:0.064526  MSE:0.002731  KL:1.620830  MAE:0.008421  RMSE:0.052263  CVRMSE:17.847020  train_loss:\n",
            "0.0027313978563197555\n",
            "R2:0.059228  MSE:0.002836  KL:0.106865  MAE:0.005774  RMSE:0.053257  CVRMSE:17.611245  test_loss:\n",
            "5.574090409887042e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064303  MSE:0.002732  KL:1.664241  MAE:0.008439  RMSE:0.052269  CVRMSE:17.849148  R2:0.064583  MSE:0.002731  KL:1.576043  MAE:0.008445  RMSE:0.052261  CVRMSE:17.846474  R2:0.064168  MSE:0.002732  KL:1.796350  MAE:0.008418  RMSE:0.052273  CVRMSE:17.850430  R2:0.064346  MSE:0.002732  KL:1.714394  MAE:0.008441  RMSE:0.052268  CVRMSE:17.848737  R2:0.065047  MSE:0.002730  KL:1.782470  MAE:0.008482  RMSE:0.052248  CVRMSE:17.842051  train_loss:\n",
            "0.002729876980947571\n",
            "R2:0.058913  MSE:0.002837  KL:0.106155  MAE:0.006180  RMSE:0.053266  CVRMSE:17.614195  test_loss:\n",
            "5.5759237337438345e-06\n",
            "R2:0.064603  MSE:0.002731  KL:1.704743  MAE:0.008488  RMSE:0.052261  CVRMSE:17.846283  R2:0.065151  MSE:0.002730  KL:1.568244  MAE:0.008501  RMSE:0.052245  CVRMSE:17.841057  R2:0.064490  MSE:0.002732  KL:1.684507  MAE:0.008433  RMSE:0.052264  CVRMSE:17.847359  R2:0.064327  MSE:0.002732  KL:1.678769  MAE:0.008466  RMSE:0.052268  CVRMSE:17.848919  R2:0.064538  MSE:0.002731  KL:1.686070  MAE:0.008441  RMSE:0.052262  CVRMSE:17.846901  train_loss:\n",
            "0.0027313612116459606\n",
            "R2:0.055627  MSE:0.002847  KL:0.099574  MAE:0.005663  RMSE:0.053359  CVRMSE:17.644922  test_loss:\n",
            "5.595549419813841e-06\n",
            "R2:0.064881  MSE:0.002730  KL:1.692004  MAE:0.008489  RMSE:0.052253  CVRMSE:17.843635  R2:0.065096  MSE:0.002730  KL:1.629322  MAE:0.008492  RMSE:0.052247  CVRMSE:17.841580  R2:0.064702  MSE:0.002731  KL:1.670928  MAE:0.008465  RMSE:0.052258  CVRMSE:17.845336  R2:0.064777  MSE:0.002731  KL:1.693296  MAE:0.008478  RMSE:0.052256  CVRMSE:17.844622  R2:0.064471  MSE:0.002732  KL:1.626087  MAE:0.008459  RMSE:0.052264  CVRMSE:17.847547  train_loss:\n",
            "0.0027315591005940333\n",
            "R2:0.056882  MSE:0.002843  KL:0.100763  MAE:0.006066  RMSE:0.053324  CVRMSE:17.633189  test_loss:\n",
            "5.588037572830648e-06\n",
            "R2:0.064721  MSE:0.002731  KL:1.709157  MAE:0.008455  RMSE:0.052257  CVRMSE:17.845154  R2:0.064393  MSE:0.002732  KL:1.598958  MAE:0.008463  RMSE:0.052266  CVRMSE:17.848282  R2:0.064486  MSE:0.002732  KL:1.668785  MAE:0.008460  RMSE:0.052264  CVRMSE:17.847396  R2:0.064551  MSE:0.002731  KL:1.594904  MAE:0.008474  RMSE:0.052262  CVRMSE:17.846776  R2:0.065195  MSE:0.002729  KL:1.621939  MAE:0.008479  RMSE:0.052244  CVRMSE:17.840639  train_loss:\n",
            "0.002729444899718322\n",
            "R2:0.057466  MSE:0.002842  KL:0.106263  MAE:0.006308  RMSE:0.053307  CVRMSE:17.627729  test_loss:\n",
            "5.584525537993261e-06\n",
            "R2:0.065585  MSE:0.002728  KL:1.696895  MAE:0.008483  RMSE:0.052233  CVRMSE:17.836912  R2:0.064765  MSE:0.002731  KL:1.728180  MAE:0.008436  RMSE:0.052256  CVRMSE:17.844737  R2:0.064085  MSE:0.002733  KL:1.662856  MAE:0.008432  RMSE:0.052275  CVRMSE:17.851226  R2:0.064873  MSE:0.002730  KL:1.667781  MAE:0.008445  RMSE:0.052253  CVRMSE:17.843709  R2:0.064537  MSE:0.002731  KL:1.614564  MAE:0.008476  RMSE:0.052262  CVRMSE:17.846913  train_loss:\n",
            "0.002731365166207848\n",
            "R2:0.056005  MSE:0.002846  KL:0.101325  MAE:0.005435  RMSE:0.053348  CVRMSE:17.641391  test_loss:\n",
            "5.593323386569179e-06\n",
            "R2:0.064874  MSE:0.002730  KL:1.614117  MAE:0.008474  RMSE:0.052253  CVRMSE:17.843695  R2:0.065428  MSE:0.002729  KL:1.678390  MAE:0.008494  RMSE:0.052238  CVRMSE:17.838408  R2:0.064734  MSE:0.002731  KL:1.566564  MAE:0.008494  RMSE:0.052257  CVRMSE:17.845029  R2:0.064831  MSE:0.002731  KL:1.667537  MAE:0.008448  RMSE:0.052254  CVRMSE:17.844106  R2:0.064284  MSE:0.002732  KL:1.615862  MAE:0.008426  RMSE:0.052270  CVRMSE:17.849326  train_loss:\n",
            "0.0027321036289597676\n",
            "R2:0.055919  MSE:0.002846  KL:0.098134  MAE:0.005489  RMSE:0.053351  CVRMSE:17.642188  test_loss:\n",
            "5.593830542514569e-06\n",
            "R2:0.064102  MSE:0.002733  KL:1.734921  MAE:0.008456  RMSE:0.052275  CVRMSE:17.851065  R2:0.065052  MSE:0.002730  KL:1.636563  MAE:0.008430  RMSE:0.052248  CVRMSE:17.841999  R2:0.065003  MSE:0.002730  KL:1.759218  MAE:0.008448  RMSE:0.052249  CVRMSE:17.842471  R2:0.064959  MSE:0.002730  KL:1.715355  MAE:0.008431  RMSE:0.052251  CVRMSE:17.842890  R2:0.065016  MSE:0.002730  KL:1.719266  MAE:0.008451  RMSE:0.052249  CVRMSE:17.842347  train_loss:\n",
            "0.002729967643436505\n",
            "R2:0.057718  MSE:0.002841  KL:0.101965  MAE:0.005889  RMSE:0.053300  CVRMSE:17.625372  test_loss:\n",
            "5.583074587849222e-06\n",
            "R2:0.065050  MSE:0.002730  KL:1.629633  MAE:0.008458  RMSE:0.052248  CVRMSE:17.842014  R2:0.064473  MSE:0.002732  KL:1.788692  MAE:0.008483  RMSE:0.052264  CVRMSE:17.847523  R2:0.064969  MSE:0.002730  KL:1.691136  MAE:0.008445  RMSE:0.052250  CVRMSE:17.842787  R2:0.064622  MSE:0.002731  KL:1.709776  MAE:0.008410  RMSE:0.052260  CVRMSE:17.846102  R2:0.064569  MSE:0.002731  KL:1.708379  MAE:0.008417  RMSE:0.052262  CVRMSE:17.846604  train_loss:\n",
            "0.0027312705061385005\n",
            "R2:0.057166  MSE:0.002843  KL:0.101119  MAE:0.005679  RMSE:0.053316  CVRMSE:17.630540  test_loss:\n",
            "5.5863880369425645e-06\n",
            "R2:0.065108  MSE:0.002730  KL:1.691200  MAE:0.008460  RMSE:0.052247  CVRMSE:17.841462  R2:0.063752  MSE:0.002734  KL:1.733131  MAE:0.008404  RMSE:0.052284  CVRMSE:17.854399  R2:0.064319  MSE:0.002732  KL:1.660698  MAE:0.008465  RMSE:0.052269  CVRMSE:17.848991  R2:0.064792  MSE:0.002731  KL:1.552379  MAE:0.008474  RMSE:0.052255  CVRMSE:17.844480  R2:0.065318  MSE:0.002729  KL:1.680250  MAE:0.008504  RMSE:0.052241  CVRMSE:17.839459  train_loss:\n",
            "0.0027290839380758308\n",
            "R2:0.057053  MSE:0.002843  KL:0.102430  MAE:0.005539  RMSE:0.053319  CVRMSE:17.631593  test_loss:\n",
            "5.5870691964109625e-06\n",
            "R2:0.064729  MSE:0.002731  KL:1.749286  MAE:0.008418  RMSE:0.052257  CVRMSE:17.845083  R2:0.065105  MSE:0.002730  KL:1.593176  MAE:0.008480  RMSE:0.052247  CVRMSE:17.841492  R2:0.064811  MSE:0.002731  KL:1.654757  MAE:0.008450  RMSE:0.052255  CVRMSE:17.844300  R2:0.064529  MSE:0.002731  KL:1.580669  MAE:0.008469  RMSE:0.052263  CVRMSE:17.846993  R2:0.064667  MSE:0.002731  KL:1.741498  MAE:0.008460  RMSE:0.052259  CVRMSE:17.845673  train_loss:\n",
            "0.002730985447720919\n",
            "R2:0.056050  MSE:0.002846  KL:0.098864  MAE:0.005795  RMSE:0.053347  CVRMSE:17.640964  test_loss:\n",
            "5.593021141463168e-06\n",
            "R2:0.065145  MSE:0.002730  KL:1.636157  MAE:0.008458  RMSE:0.052245  CVRMSE:17.841112  R2:0.064358  MSE:0.002732  KL:1.728840  MAE:0.008432  RMSE:0.052267  CVRMSE:17.848619  R2:0.064692  MSE:0.002731  KL:1.708726  MAE:0.008437  RMSE:0.052258  CVRMSE:17.845431  R2:0.064572  MSE:0.002731  KL:1.697564  MAE:0.008465  RMSE:0.052261  CVRMSE:17.846583  R2:0.064802  MSE:0.002731  KL:1.730594  MAE:0.008440  RMSE:0.052255  CVRMSE:17.844384  train_loss:\n",
            "0.0027305908398629515\n",
            "R2:0.057932  MSE:0.002840  KL:0.105275  MAE:0.005720  RMSE:0.053294  CVRMSE:17.623372  test_loss:\n",
            "5.581813472600964e-06\n",
            "R2:0.065320  MSE:0.002729  KL:1.751046  MAE:0.008470  RMSE:0.052241  CVRMSE:17.839443  R2:0.064585  MSE:0.002731  KL:1.711296  MAE:0.008454  RMSE:0.052261  CVRMSE:17.846452  R2:0.064668  MSE:0.002731  KL:1.682905  MAE:0.008424  RMSE:0.052259  CVRMSE:17.845662  R2:0.064739  MSE:0.002731  KL:1.598468  MAE:0.008507  RMSE:0.052257  CVRMSE:17.844985  R2:0.063797  MSE:0.002734  KL:1.720436  MAE:0.008428  RMSE:0.052283  CVRMSE:17.853971  train_loss:\n",
            "0.002733525740380088\n",
            "R2:0.056474  MSE:0.002845  KL:0.102183  MAE:0.005503  RMSE:0.053335  CVRMSE:17.637004  test_loss:\n",
            "5.590517365418668e-06\n",
            "R2:0.064945  MSE:0.002730  KL:1.726274  MAE:0.008452  RMSE:0.052251  CVRMSE:17.843025  R2:0.064112  MSE:0.002733  KL:1.682746  MAE:0.008458  RMSE:0.052274  CVRMSE:17.850965  R2:0.064804  MSE:0.002731  KL:1.749214  MAE:0.008444  RMSE:0.052255  CVRMSE:17.844369  R2:0.064830  MSE:0.002731  KL:1.704940  MAE:0.008470  RMSE:0.052254  CVRMSE:17.844121  R2:0.064242  MSE:0.002732  KL:1.632634  MAE:0.008466  RMSE:0.052271  CVRMSE:17.849726  train_loss:\n",
            "0.0027322262019477623\n",
            "R2:0.053657  MSE:0.002853  KL:0.098915  MAE:0.005327  RMSE:0.053415  CVRMSE:17.663316  test_loss:\n",
            "5.607323789746236e-06\n",
            "R2:0.064221  MSE:0.002732  KL:1.587938  MAE:0.008453  RMSE:0.052271  CVRMSE:17.849931  R2:0.064542  MSE:0.002731  KL:1.664459  MAE:0.008464  RMSE:0.052262  CVRMSE:17.846861  R2:0.064712  MSE:0.002731  KL:1.650618  MAE:0.008474  RMSE:0.052258  CVRMSE:17.845247  R2:0.063966  MSE:0.002733  KL:1.679771  MAE:0.008440  RMSE:0.052278  CVRMSE:17.852359  R2:0.065092  MSE:0.002730  KL:1.665521  MAE:0.008459  RMSE:0.052247  CVRMSE:17.841617  train_loss:\n",
            "0.002729744173084756\n",
            "R2:0.057640  MSE:0.002841  KL:0.105145  MAE:0.005484  RMSE:0.053302  CVRMSE:17.626106  test_loss:\n",
            "5.583582497864061e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2:0.064688  MSE:0.002731  KL:1.731077  MAE:0.008439  RMSE:0.052258  CVRMSE:17.845475  R2:0.064379  MSE:0.002732  KL:1.767459  MAE:0.008429  RMSE:0.052267  CVRMSE:17.848424  R2:0.064461  MSE:0.002732  KL:1.777455  MAE:0.008439  RMSE:0.052265  CVRMSE:17.847634  R2:0.064899  MSE:0.002730  KL:1.683874  MAE:0.008465  RMSE:0.052252  CVRMSE:17.843460  R2:0.063806  MSE:0.002733  KL:1.786041  MAE:0.008441  RMSE:0.052283  CVRMSE:17.853883  train_loss:\n",
            "0.002733499015558268\n",
            "R2:0.056011  MSE:0.002846  KL:0.099016  MAE:0.005763  RMSE:0.053348  CVRMSE:17.641328  test_loss:\n",
            "5.593251218773543e-06\n",
            "R2:0.064669  MSE:0.002731  KL:1.590624  MAE:0.008481  RMSE:0.052259  CVRMSE:17.845651  R2:0.064686  MSE:0.002731  KL:1.735661  MAE:0.008474  RMSE:0.052258  CVRMSE:17.845490  R2:0.065101  MSE:0.002730  KL:1.621649  MAE:0.008488  RMSE:0.052247  CVRMSE:17.841530  R2:0.064795  MSE:0.002731  KL:1.760341  MAE:0.008435  RMSE:0.052255  CVRMSE:17.844450  R2:0.064473  MSE:0.002732  KL:1.624434  MAE:0.008470  RMSE:0.052264  CVRMSE:17.847527  train_loss:\n",
            "0.0027315529838789813\n",
            "R2:0.056022  MSE:0.002846  KL:0.102847  MAE:0.005475  RMSE:0.053348  CVRMSE:17.641225  test_loss:\n",
            "5.593209324628976e-06\n",
            "R2:0.064888  MSE:0.002730  KL:1.784816  MAE:0.008441  RMSE:0.052253  CVRMSE:17.843569  R2:0.064476  MSE:0.002732  KL:1.577582  MAE:0.008478  RMSE:0.052264  CVRMSE:17.847492  R2:0.064659  MSE:0.002731  KL:1.685556  MAE:0.008480  RMSE:0.052259  CVRMSE:17.845748  R2:0.064667  MSE:0.002731  KL:1.729333  MAE:0.008454  RMSE:0.052259  CVRMSE:17.845675  R2:0.064767  MSE:0.002731  KL:1.715840  MAE:0.008494  RMSE:0.052256  CVRMSE:17.844719  train_loss:\n",
            "0.002730693440230334\n",
            "R2:0.057494  MSE:0.002842  KL:0.102949  MAE:0.005556  RMSE:0.053306  CVRMSE:17.627470  test_loss:\n",
            "5.584443543307622e-06\n",
            "R2:0.065348  MSE:0.002729  KL:1.634307  MAE:0.008484  RMSE:0.052240  CVRMSE:17.839170  R2:0.064330  MSE:0.002732  KL:1.759439  MAE:0.008438  RMSE:0.052268  CVRMSE:17.848886  R2:0.064924  MSE:0.002730  KL:1.633864  MAE:0.008442  RMSE:0.052252  CVRMSE:17.843219  R2:0.065477  MSE:0.002729  KL:1.622608  MAE:0.008508  RMSE:0.052236  CVRMSE:17.837939  R2:0.064564  MSE:0.002731  KL:1.742164  MAE:0.008451  RMSE:0.052262  CVRMSE:17.846652  train_loss:\n",
            "0.0027312850960671386\n",
            "R2:0.057167  MSE:0.002843  KL:0.102265  MAE:0.005604  RMSE:0.053316  CVRMSE:17.630524  test_loss:\n",
            "5.5863818494756735e-06\n",
            "R2:0.064631  MSE:0.002731  KL:1.677863  MAE:0.008460  RMSE:0.052260  CVRMSE:17.846020  R2:0.064964  MSE:0.002730  KL:1.632691  MAE:0.008458  RMSE:0.052251  CVRMSE:17.842840  R2:0.064466  MSE:0.002732  KL:1.655661  MAE:0.008454  RMSE:0.052264  CVRMSE:17.847595  R2:0.064554  MSE:0.002731  KL:1.742515  MAE:0.008480  RMSE:0.052262  CVRMSE:17.846749  R2:0.064445  MSE:0.002732  KL:1.559992  MAE:0.008456  RMSE:0.052265  CVRMSE:17.847793  train_loss:\n",
            "0.002731634258284535\n",
            "R2:0.057446  MSE:0.002842  KL:0.103365  MAE:0.005451  RMSE:0.053308  CVRMSE:17.627915  test_loss:\n",
            "5.584739565951664e-06\n",
            "R2:0.064623  MSE:0.002731  KL:1.783031  MAE:0.008466  RMSE:0.052260  CVRMSE:17.846094  R2:0.064554  MSE:0.002731  KL:1.629273  MAE:0.008455  RMSE:0.052262  CVRMSE:17.846755  R2:0.064558  MSE:0.002731  KL:1.640603  MAE:0.008466  RMSE:0.052262  CVRMSE:17.846717  R2:0.064830  MSE:0.002731  KL:1.723560  MAE:0.008484  RMSE:0.052254  CVRMSE:17.844116  R2:0.064219  MSE:0.002732  KL:1.539469  MAE:0.008470  RMSE:0.052271  CVRMSE:17.849949  train_loss:\n",
            "0.0027322943243339557\n",
            "R2:0.057344  MSE:0.002842  KL:0.101767  MAE:0.005935  RMSE:0.053311  CVRMSE:17.628876  test_loss:\n",
            "5.585302129990719e-06\n",
            "R2:0.064564  MSE:0.002731  KL:1.743849  MAE:0.008482  RMSE:0.052262  CVRMSE:17.846660  R2:0.064716  MSE:0.002731  KL:1.718560  MAE:0.008428  RMSE:0.052257  CVRMSE:17.845206  R2:0.064114  MSE:0.002733  KL:1.622179  MAE:0.008466  RMSE:0.052274  CVRMSE:17.850952  R2:0.064451  MSE:0.002732  KL:1.828933  MAE:0.008446  RMSE:0.052265  CVRMSE:17.847737  R2:0.064592  MSE:0.002731  KL:1.821282  MAE:0.008432  RMSE:0.052261  CVRMSE:17.846387  train_loss:\n",
            "0.0027312041319765786\n",
            "R2:0.057022  MSE:0.002843  KL:0.102018  MAE:0.005745  RMSE:0.053320  CVRMSE:17.631887  test_loss:\n",
            "5.587234231942514e-06\n",
            "R2:0.064571  MSE:0.002731  KL:1.759261  MAE:0.008478  RMSE:0.052262  CVRMSE:17.846588  R2:0.064411  MSE:0.002732  KL:1.588577  MAE:0.008465  RMSE:0.052266  CVRMSE:17.848112  R2:0.064917  MSE:0.002730  KL:1.671352  MAE:0.008462  RMSE:0.052252  CVRMSE:17.843292  R2:0.065154  MSE:0.002730  KL:1.579266  MAE:0.008482  RMSE:0.052245  CVRMSE:17.841026  R2:0.065069  MSE:0.002730  KL:1.634542  MAE:0.008469  RMSE:0.052248  CVRMSE:17.841833  train_loss:\n",
            "0.0027298102438719535\n",
            "R2:0.055951  MSE:0.002846  KL:0.099865  MAE:0.005352  RMSE:0.053350  CVRMSE:17.641894  test_loss:\n",
            "5.593656633134764e-06\n",
            "R2:0.065219  MSE:0.002729  KL:1.652297  MAE:0.008494  RMSE:0.052243  CVRMSE:17.840410  R2:0.064630  MSE:0.002731  KL:1.746327  MAE:0.008460  RMSE:0.052260  CVRMSE:17.846024  R2:0.064814  MSE:0.002731  KL:1.671631  MAE:0.008462  RMSE:0.052255  CVRMSE:17.844267  R2:0.064231  MSE:0.002732  KL:1.745607  MAE:0.008453  RMSE:0.052271  CVRMSE:17.849828  R2:0.064966  MSE:0.002730  KL:1.619141  MAE:0.008491  RMSE:0.052250  CVRMSE:17.842818  train_loss:\n",
            "0.0027301116588341933\n",
            "R2:0.058653  MSE:0.002838  KL:0.109905  MAE:0.005779  RMSE:0.053274  CVRMSE:17.616628  test_loss:\n",
            "5.577506289142657e-06\n",
            "R2:0.065372  MSE:0.002729  KL:1.634262  MAE:0.008498  RMSE:0.052239  CVRMSE:17.838950  R2:0.064861  MSE:0.002730  KL:1.685126  MAE:0.008458  RMSE:0.052253  CVRMSE:17.843819  R2:0.064940  MSE:0.002730  KL:1.667819  MAE:0.008472  RMSE:0.052251  CVRMSE:17.843064  R2:0.065240  MSE:0.002729  KL:1.740805  MAE:0.008500  RMSE:0.052243  CVRMSE:17.840208  R2:0.064854  MSE:0.002730  KL:1.688191  MAE:0.008462  RMSE:0.052254  CVRMSE:17.843889  train_loss:\n",
            "0.002730439365991501\n",
            "R2:0.057309  MSE:0.002842  KL:0.102537  MAE:0.005812  RMSE:0.053312  CVRMSE:17.629197  test_loss:\n",
            "5.5855147712000095e-06\n",
            "Model 0 has MSE on validset:0.030515  Model 0 has KL on validset:0.024179  Model 0 has ATE error on validset:1.008857  "
          ]
        }
      ],
      "source": [
        "from uuid import RFC_4122\n",
        "import torch\n",
        "import math\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "from torch_geometric.utils import add_self_loops,degree\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import ssl\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net,self).__init__()\n",
        "        self.gat1=GATConv(in_channels=25,out_channels=8,heads=8,dropout=0.6)\n",
        "        self.gat2=GATConv(in_channels=64,out_channels=10,heads=1,dropout=0.6)\n",
        "        self.f1 = torch.nn.Linear(140,32)\n",
        "        self.f2 = torch.nn.Linear(32,1)\n",
        "    def forward(self,data):\n",
        "        x,edge_index=data.x, data.edge_index\n",
        "        x=self.gat1(x,edge_index)\n",
        "        x=self.gat2(x,edge_index)\n",
        "        x=x.reshape(-1,140)\n",
        "        x = self.f1(x)\n",
        "        x = self.f2(x)\n",
        "        return x\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "def train():\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in train_loader:\n",
        "        loss = 0\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        label = data.y.to(device)\n",
        "        loss = crit(output, label)\n",
        "        loss.backward()\n",
        "        loss_all += data.num_graphs * loss.item()\n",
        "        optimizer.step()\n",
        "        y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "        y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "    \n",
        "    \n",
        "    loss=loss_all/len(Mydata_train)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2,mse,kl\n",
        "\n",
        "\n",
        "def val():\n",
        "    model.eval()\n",
        "    y_actual = []\n",
        "    y_predicted = []\n",
        "    loss_all=0\n",
        "    for data in test_loader:\n",
        "      loss = 0\n",
        "      data = data.to(device)\n",
        "      output = model(data)\n",
        "      label = data.y.to(device)\n",
        "      y_actual +=(label).cpu().detach().ravel().tolist()\n",
        "      y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "      loss = crit(output, label)\n",
        "      loss_all += loss.item()\n",
        "\n",
        "    \n",
        "    loss = loss_all / len(Mydata_test)\n",
        "    r2=R2(y_predicted, y_actual)\n",
        "    mse = MSE(y_predicted, y_actual)\n",
        "    kl=kl_divergence(y_predicted, y_actual)\n",
        "\n",
        "    print(\"R2:%f\" % (R2(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MSE:%f\" % (MSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"KL:%f\" % (kl_divergence(y_predicted, y_actual)),end='  ')\n",
        "    print(\"MAE:%f\" % (MAE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"RMSE:%f\" % (RMSE(y_predicted, y_actual)),end='  ')\n",
        "    print(\"CVRMSE:%f\" % (CVRMSE(y_predicted, y_actual)),end='  ')\n",
        "\n",
        "    return loss,r2, mse,kl\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 256\n",
        "batch_size = 512\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=5e-4)\n",
        "crit = F.mse_loss\n",
        "train_loader = DataLoader(Mydata_train, batch_size=batch_size)\n",
        "test_loader = DataLoader(Mydata_test, batch_size=512)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    loss,r2,mse,kl=train()\n",
        "    if epoch %5==0:      \n",
        "        print('train_loss:')\n",
        "        print(loss)\n",
        "\n",
        "        loss,r2,mse,kl=val()\n",
        "        print('test_loss:')\n",
        "        print(loss)\n",
        "\n",
        "y_predicted = []\n",
        "for data in test_loader:\n",
        "    loss = 0\n",
        "    data = data.to(device)\n",
        "    output = model(data)\n",
        "    label = data.y.to(device)\n",
        "    y_predicted +=(output).cpu().detach().ravel().tolist()\n",
        "df_test[\"y_hat\"]=y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2txUFKIhFkOA",
        "outputId": "e5f8effd-aa98-40fa-b8e3-610216200cdb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auuc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y_hat</th>\n",
              "      <td>0.880700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random</th>\n",
              "      <td>0.499185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            auuc\n",
              "y_hat   0.880700\n",
              "Random  0.499185"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# estimate area under the uplift curve (AUUC)\n",
        "from causalml.metrics import *\n",
        "uplift=df_test.copy()\n",
        "tau_hat=pd.concat([t0,t1], axis=0, join=\"inner\")\n",
        "uplift=pd.concat([uplift,tau_hat], axis=1, join=\"inner\")\n",
        "uplift = uplift.loc[:,~uplift.columns.duplicated()]\n",
        "\n",
        "auuc=auuc_score(uplift, outcome_col='y', treatment_col='T', treatment_effect_col='tau')\n",
        "gat_auuc=pd.DataFrame(auuc[[\"y_hat\",\"Random\"]],columns=['auuc'])\n",
        "gat_auuc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8sZB2KGFkOB",
        "outputId": "4fd406c1-016d-417e-f668-4e71ceada1e2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>AUUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>S Learner(LR)</th>\n",
              "      <td>0.497983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S Learner(XGB)</th>\n",
              "      <td>0.875572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>S Learner(LGBM)</th>\n",
              "      <td>0.883033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct)</th>\n",
              "      <td>0.501865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct+Feature)</th>\n",
              "      <td>0.721959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GCN (Struct+Causal Weighting)</th>\n",
              "      <td>0.732616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct)</th>\n",
              "      <td>0.544286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct+Feature)</th>\n",
              "      <td>0.84763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GAT (Struct+Causal Weighting)</th>\n",
              "      <td>0.8807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   AUUC\n",
              "S Learner(LR)                  0.497983\n",
              "S Learner(XGB)                 0.875572\n",
              "S Learner(LGBM)                0.883033\n",
              "GCN (Struct)                   0.501865\n",
              "GCN (Struct+Feature)           0.721959\n",
              "GCN (Struct+Causal Weighting)  0.732616\n",
              "GAT (Struct)                   0.544286\n",
              "GAT (Struct+Feature)            0.84763\n",
              "GAT (Struct+Causal Weighting)    0.8807"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result=pd.DataFrame(columns=[['AUUC']])\n",
        "result.loc['S Learner(LR)','AUUC']=0.497983\n",
        "result.loc['S Learner(XGB)','AUUC']=0.875572\n",
        "result.loc['S Learner(LGBM)','AUUC']=0.883033\n",
        "\n",
        "result.loc['GCN (Struct)','AUUC']=0.501865\n",
        "result.loc['GCN (Struct+Feature)','AUUC']=0.721959\n",
        "result.loc['GCN (Struct+Causal Weighting)','AUUC']=gcn_auuc.loc[\"y_hat\"].values\n",
        "\n",
        "result.loc['GAT (Struct)','AUUC']=0.544286\n",
        "result.loc['GAT (Struct+Feature)','AUUC']=0.847630\n",
        "result.loc['GAT (Struct+Causal Weighting)','AUUC']=gat_auuc.loc[\"y_hat\"].values\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6E9txoQFkOB",
        "outputId": "34e37ced-f84c-44f7-a6a2-1b5ddf856ffe"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>GCN</th>\n",
              "      <th>GAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>equal weighting</th>\n",
              "      <td>0.721959</td>\n",
              "      <td>0.84763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>causal weighting</th>\n",
              "      <td>0.732616</td>\n",
              "      <td>0.8807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       GCN      GAT\n",
              "equal weighting   0.721959  0.84763\n",
              "causal weighting  0.732616   0.8807"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result=pd.DataFrame(columns=[['GCN','GAT']])\n",
        "result.loc['equal weighting','GCN']=0.721959\n",
        "result.loc['equal weighting','GAT']=0.847630\n",
        "\n",
        "result.loc['causal weighting','GCN']=gcn_auuc.loc[\"y_hat\"].values\n",
        "result.loc['causal weighting','GAT']=gat_auuc.loc[\"y_hat\"].values\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "cd9ce680b654e493fbbd5797259a0b454ac6d0effeb2d1358682b4d9ed73e0e9"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}